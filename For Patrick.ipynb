{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ce4ecb",
   "metadata": {},
   "source": [
    "### Install beta version of DICES client library\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/cwf2/dices-client.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389d720",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "import pandas as pd\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d2aaa",
   "metadata": {},
   "source": [
    "### Set up connection to DICES database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices = DicesAPI(\n",
    "    dices_api = 'http://csa20211203-005.uni-rostock.de/api',\n",
    "    cts_api = 'https://scaife-cts.perseus.org/api/cts',\n",
    "    progress_class = NotebookPBar,\n",
    "    logfile='dices.log',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd7520",
   "metadata": {},
   "source": [
    "### Download Lucan's speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = dices.getSpeeches(author_name='Lucan', progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940c5ee",
   "metadata": {},
   "source": [
    "A super quick look at what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([dict(\n",
    "    urn = s.urn,\n",
    "    first_line = s.l_fi,\n",
    "    last_line = s.l_la,\n",
    "    speaker = ', '.join([inst.name for inst in s.spkr]),\n",
    "    addressee = ', '.join([inst.name for inst in s.addr]),\n",
    ") for s in speeches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47db61",
   "metadata": {},
   "source": [
    "### Getting the text\n",
    "\n",
    "The speech records in DICES only have metadata; to get the text, we use CTS to request each passage from Perseus. I'm going to tack the passages onto the existing speech objects.\n",
    "\n",
    "One limitation of DICES right now: **line is the finest granularity we have for beginnings and endings.** So we're picking up *verba dicendi* and other extra material in speeches that start or end partway through a line. For a lot of our Greek texts it isn't an issue; and for some other authors there are quotation marks or `<q>` tags in the xml that let us find the edges of the speech, but not for Lucan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes long enough that I like a progress bar\n",
    "pbar = NotebookPBar(max=len(speeches))\n",
    "\n",
    "for s in speeches:\n",
    "    try:\n",
    "        s.cts_passage = s.getCTS()\n",
    "    except:\n",
    "        print('Failed to get', s)\n",
    "        s.cts_passage = None\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69627b56",
   "metadata": {},
   "source": [
    "#### Whole speeches\n",
    "\n",
    "The simplest way to get the text is the `text` attribute of the cts passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([dict(\n",
    "    first_line = s.l_fi,\n",
    "    last_line = s.l_la,\n",
    "    speaker = ', '.join([inst.name for inst in s.spkr]),\n",
    "    addressee = ', '.join([inst.name for inst in s.addr]),\n",
    "    text = s.cts_passage.text,\n",
    ") for s in speeches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13bfe7",
   "metadata": {},
   "source": [
    "#### Line-by-line\n",
    "\n",
    "This is the best way I've come up with to parse the cts passages into lines. **üíÅüèª‚Äç‚ôÇÔ∏è Any suggestions here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//{http://www.tei-c.org/ns/1.0}l'\n",
    "\n",
    "for s in speeches:\n",
    "    s.verse_array = [dict(\n",
    "        n = l.get('n'), \n",
    "        text = ''.join(l.itertext()),\n",
    "    ) for l in s.cts_passage.xml.getroottree().findall(xpath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e999f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(speeches[-1].verse_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcbd4cf",
   "metadata": {},
   "source": [
    "### NLP with CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba7424",
   "metadata": {},
   "source": [
    "#### Working with language-specific pipelines\n",
    "\n",
    "This isn't necessary when we're just looking at Lucan, but I'm including it to show my more general workflow, in combination with the `.lang` attribute of DICES Speech objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368166f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_nlp = dict(\n",
    "    latin = NLP('lat', suppress_banner=True),\n",
    "    greek = NLP('grc', suppress_banner=True),\n",
    ")\n",
    "\n",
    "# remove LatinLexiconProcess\n",
    "#     - assumes it's the last process\n",
    "cltk_nlp['latin'].pipeline.processes = cltk_nlp['latin'].pipeline.processes[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12da49",
   "metadata": {},
   "source": [
    "#### Parsing the whole text of each speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a long time\n",
    "pbar = NotebookPBar(max=len(speeches))\n",
    "\n",
    "for s in speeches:\n",
    "    s.cltk_doc = cltk_nlp[s.lang](s.cts_passage.text)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90468f10",
   "metadata": {},
   "source": [
    "üíÅüèª‚Äç‚ôÇÔ∏è Questions:\n",
    "\n",
    " - ~Can I leave out of the pipeline whatever is retrieving all the dictionary entries?~ ‚úÖ\n",
    " - Can I make this any faster? ‚úÖ (see prev)\n",
    " - Can I get the attributes `index_char_start` and `index_char_stop`?\n",
    " - ~Should I be breaking this up into sentences?~\n",
    "  - Seems like\n",
    "    - it gets broken into sentences anyway\n",
    "    - it's slightly faster to do it all at once\n",
    " - Would it work on individual lines, even if they're not grammatically complete?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3033b",
   "metadata": {},
   "source": [
    "#### Identifying line of origin for each token in the cltk document\n",
    "\n",
    "Yeah, this seems ugly. But it works, and I want to leave time for my fanfiction project this evening...\n",
    "\n",
    "Should create a record for every token in the `cltk_doc.words` list, giving the index of its line within `verse_array`, the canonical line number, and start and end positions within the string representation of the verse. I have a feeling that tags like `<note>` will have to be pruned for this to be really robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53576a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_loci(cltk_doc, verse_array):\n",
    "    '''Look up each string in the verse array, return locs'''\n",
    "    \n",
    "    # we're going to look up each token string from the full speech\n",
    "    #     in the verse-line array, crossing off each as we go\n",
    "    \n",
    "    # these are the strings to lookup\n",
    "    tok_strings = [w.string for w in cltk_doc.words]\n",
    "    \n",
    "    # this is the current position in the verse array\n",
    "    #     and  string of the verse text\n",
    "    verse_i = 0\n",
    "    last_good_i = 0\n",
    "    working_text = verse_array[verse_i]['text']\n",
    "    \n",
    "    # holds results\n",
    "    token_loc = []\n",
    "\n",
    "    for tok in tok_strings:        \n",
    "        while True:\n",
    "        \n",
    "            if working_text is None:\n",
    "                working_text = ''\n",
    "        \n",
    "            # look for the word with boundaries\n",
    "            regex = re.compile(r'\\b' + re.escape(tok) + r'\\b')\n",
    "            m = regex.search(working_text)\n",
    "            \n",
    "            # then try without boundaries (punctuation, enclitics?)\n",
    "            if m is None:\n",
    "                regex = re.compile(re.escape(tok))\n",
    "                m = regex.search(working_text)           \n",
    "                \n",
    "                # still no? then try next line\n",
    "                if m is None:\n",
    "                    verse_i += 1\n",
    "                \n",
    "                    if verse_i < len(verse_array):\n",
    "                        working_text = verse_array[verse_i]['text']                    \n",
    "                        continue\n",
    "                        \n",
    "                    # if we're at the end of the verse array, give up\n",
    "                    else:\n",
    "                        token_loc.append(dict())\n",
    "                        \n",
    "                        # go back to the last line that matched something\n",
    "                        verse_i = last_good_i\n",
    "                        break\n",
    "\n",
    "            # if we found a match, cross it off\n",
    "            offset = m.start()\n",
    "            working_text = regex.sub('üßÄ'*len(tok), working_text, count=1)\n",
    "\n",
    "            token_loc.append(dict(\n",
    "                i = verse_i,\n",
    "                start = offset,\n",
    "                end = offset + len(tok),\n",
    "                n = verse_array[verse_i]['n'],\n",
    "            ))\n",
    "            break\n",
    "    \n",
    "    return token_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5797a",
   "metadata": {},
   "source": [
    "Add this info to every speech, so I can use it later to correlate the parsed tokens in `cltk_doc.words` with the loci and text of `verse_array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in speeches:\n",
    "    s.token_loc = get_word_loci(s.cltk_doc, s.verse_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee4418",
   "metadata": {},
   "source": [
    "Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f882cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(speeches[-1].token_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e879c",
   "metadata": {},
   "source": [
    "### Colour all the verbs\n",
    "\n",
    "A test case. Can I pick out tokens based on a cltk feature like part of speech, and then use that info in a line-based treatment of the passage, such as displaying one line per row in an HTML table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_table_coloured_verbs(s):\n",
    "    '''Produce an HTML table of the speech text,\n",
    "          with all verbs highlighted.\n",
    "    '''\n",
    "    \n",
    "    rows = deepcopy(s.verse_array)\n",
    "    \n",
    "    # work through the tokens backwards, so when we modify\n",
    "    #     the line strings, our start/end offsets aren't\n",
    "    #     messed up by earlier words\n",
    "    \n",
    "    for tok_index in reversed(range(len(s.cltk_doc.words))):\n",
    "        \n",
    "        tok = s.cltk_doc.words[tok_index]\n",
    "        verse_index = s.token_loc[tok_index]['i']\n",
    "        \n",
    "        if tok.pos.name == 'verb' and tok.features['VerbForm'][0].name == 'finite':\n",
    "            start = s.token_loc[tok_index]['start']\n",
    "            end = s.token_loc[tok_index]['end']\n",
    "            text = rows[verse_index]['text']\n",
    "            \n",
    "            rows[verse_index]['text'] = '{before}<span class=\"d-inline-block\" data-bs-toggle=\"tooltip\" title=\"{meta}\"><span style=\"color:red\">{middle}</span></span>{after}'.format(\n",
    "                meta = f'Lemma: {tok.lemma}\\n' + ', '.join(f'{k}:{v}' for k,v in tok.features.all()),\n",
    "                before = text[:start],\n",
    "                middle = text[start:end],\n",
    "                after = text[end:],\n",
    "            )\n",
    "    \n",
    "    rows = [f'<tr><td>{row[\"n\"]}</td><td style=\"text-align:left\">{row[\"text\"]}</td></tr>' \n",
    "                for row in rows]\n",
    "    \n",
    "    return ('<table class=\"table table-border\" style=\"width:80%\">' +\n",
    "             '<thead><tr><th style=\"width:5em\">line</th><th style=\"text-align:left\">text</th></tr></thead>'+ \n",
    "             '<tbody>' +\n",
    "                ''.join(rows) +\n",
    "             '</tbody>' +\n",
    "            '</table>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in speeches:\n",
    "    display(\n",
    "        HTML(f'<h3>{s.work.title} {s.l_range}</h3>'),\n",
    "        HTML(html_table_coloured_verbs(s))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd23173",
   "metadata": {},
   "source": [
    "There are some odd mistakes in here: I've added tooltips above to help examine the details. Below I look at all the words in one speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('max_rows', None, 'max_columns', None, 'max_colwidth', 100):\n",
    "    display(pd.DataFrame(dict(\n",
    "    string = w.string,\n",
    "    pos = w.pos.name,\n",
    "    lemma = w.lemma,\n",
    "    features = '; '.join(f'{k}:{v[0].name}' for k, v in w.features.all())\n",
    ") for w in speeches[2].cltk_doc.words))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
