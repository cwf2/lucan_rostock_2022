{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ce4ecb",
   "metadata": {},
   "source": [
    "### Install beta version of DICES client library\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/cwf2/dices-client.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389d720",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d2aaa",
   "metadata": {},
   "source": [
    "### Set up connection to DICES database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices = DicesAPI(\n",
    "    dices_api = 'http://csa20211203-005.uni-rostock.de/api',\n",
    "    cts_api = 'https://scaife-cts.perseus.org/api/cts',\n",
    "    progress_class = NotebookPBar,\n",
    "    logfile='dices.log',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd7520",
   "metadata": {},
   "source": [
    "### Download Lucan's speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = dices.getSpeeches(author_name='Lucan', progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940c5ee",
   "metadata": {},
   "source": [
    "A super quick look at what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([dict(\n",
    "    urn = s.urn,\n",
    "    first_line = s.l_fi,\n",
    "    last_line = s.l_la,\n",
    "    speaker = ', '.join([inst.name for inst in s.spkr]),\n",
    "    addressee = ', '.join([inst.name for inst in s.addr]),\n",
    ") for s in speeches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47db61",
   "metadata": {},
   "source": [
    "### Getting the text\n",
    "\n",
    "The speech records in DICES only have metadata; to get the text, we use CTS to request each passage from Perseus. I'm going to tack the passages onto the existing speech objects.\n",
    "\n",
    "One limitation of DICES right now: **line is the finest granularity we have for beginnings and endings.** So we're picking up *verba dicendi* and other extra material in speeches that start or end partway through a line. For a lot of our Greek texts it isn't an issue; and for some other authors there are quotation marks or `<q>` tags in the xml that let us find the edges of the speech, but not for Lucan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes long enough that I like a progress bar\n",
    "pbar = NotebookPBar(max=len(speeches))\n",
    "\n",
    "for s in speeches:\n",
    "    try:\n",
    "        s.cts_passage = s.getCTS()\n",
    "    except:\n",
    "        print('Failed to get', s)\n",
    "        s.cts_passage = None\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69627b56",
   "metadata": {},
   "source": [
    "#### Whole speeches\n",
    "\n",
    "The simplest way to get the text is the `text` attribute of the cts passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([dict(\n",
    "    first_line = s.l_fi,\n",
    "    last_line = s.l_la,\n",
    "    speaker = ', '.join([inst.name for inst in s.spkr]),\n",
    "    addressee = ', '.join([inst.name for inst in s.addr]),\n",
    "    text = s.cts_passage.text,\n",
    ") for s in speeches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13bfe7",
   "metadata": {},
   "source": [
    "#### Line-by-line\n",
    "\n",
    "This is the best way I've come up with to parse the cts passages into lines. **üíÅüèª‚Äç‚ôÇÔ∏è Any suggestions here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//{http://www.tei-c.org/ns/1.0}l'\n",
    "\n",
    "for s in speeches:\n",
    "    s.verse_array = [dict(\n",
    "        n = l.get('n'), \n",
    "        text = l.text,\n",
    "    ) for l in s.cts_passage.xml.getroottree().findall(xpath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e999f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(speeches[2].verse_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcbd4cf",
   "metadata": {},
   "source": [
    "### NLP with CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba7424",
   "metadata": {},
   "source": [
    "#### Working with language-specific pipelines\n",
    "\n",
    "This isn't necessary when we're just looking at Lucan, but I'm including it to show my more general workflow, in combination with the `.lang` attribute of DICES Speech objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368166f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_nlp = dict(\n",
    "    latin = NLP('lat'),\n",
    "    greek = NLP('grc'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12da49",
   "metadata": {},
   "source": [
    "#### Parsing the whole text of each speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a long time and I've never actually run it all the way through...\n",
    "pbar = NotebookPBar(max=len(speeches))\n",
    "\n",
    "for s in speeches:\n",
    "    s.cltk_doc = cltk_nlp[s.lang](s.cts_passage.text)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90468f10",
   "metadata": {},
   "source": [
    "üíÅüèª‚Äç‚ôÇÔ∏è Questions:\n",
    "\n",
    " - Can I leave out of the pipeline whatever is retrieving all the dictionary entries?\n",
    " - Can I make this any faster?\n",
    " - I notice that the words have placeholder attributes for the start and end positions in the string. Can I turn these on?\n",
    " - Should I be breaking this up into sentences?\n",
    " - Would it work on individual lines, even if they're not grammatically complete?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3033b",
   "metadata": {},
   "source": [
    "#### Breaking into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.sentence.lat import LatinPunktSentenceTokenizer\n",
    "splitter = LatinPunktSentenceTokenizer()\n",
    "\n",
    "for s in speeches:\n",
    "    s.sentences = splitter.tokenize(s.cts_passage.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
